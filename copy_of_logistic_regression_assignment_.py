# -*- coding: utf-8 -*-
"""Copy of logistic regression assignment .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HEyDrCF26q8JWa6xrlJsXxztqvgJeO6A
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('Titanic_train.csv')

df

df.head()

df.info()

df.describe()

df.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt
plt.hist(df['Age'], bins=20, edgecolor='black')
plt.figure(figsize=(8,6))
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('count')
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='Pclass', y='Fare', data=df)
plt.title('Fare Distribution by Passenger Class')
plt.xlabel('Passenger Class')
plt.ylabel('Fare')
plt.show()

sns.pairplot(df)
plt.show()

numerical_features = df.select_dtypes(include=np.number).columns
correlation_matrix = df[numerical_features].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Titanic Dataset')
plt.show()



import pandas as pd
from sklearn.model_selection import train_test_split # Import train_test_split
df = pd.read_csv('Titanic_train.csv')
X = df[['Pclass', 'Age', 'Fare']]
y = df['Survived']

X['Age'].fillna(X['Age'].mean(), inplace=True)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed

import pandas as pd
df = pd.read_csv('Titanic_train.csv')

categorical_features = df.select_dtypes(include=['object']).columns
print(categorical_features)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X = df[['Pclass', 'Age', 'Fare']]
y = df['Survived']
X['Age'].fillna(X['Age'].mean(), inplace=True)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X = df[['Pclass', 'Age', 'Fare']]

model = LogisticRegression()
model.fit(X_train, y_train)
print("Model trained successfully!")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
print(f"ROC-AUC score: {roc_auc}")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Calculate fpr and tpr using roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

print(model.coef_)
print(model.intercept_)

feature_names = X.columns
coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': model.coef_[0]})
print(coefficients)

"""Feature significance refers to the importance or influence of a particular feature in predicting the target variable. In the Titanic dataset, we want to understand which features (like passenger class, age, fare, etc.) played a crucial role in determining a passenger's survival.

deploment model:
"""

!pip install streamlit==1.28.1

import pandas as pd
pd.to_pickle(model, 'titanic_model.pkl')

!streamlit run app.py

from pickle import dump
dump(model, open('titanic_model.pkl', 'wb'))

from pickle import load

loaded_model = load(open('titanic_model.pkl', 'rb'))

from pickle import load

# Load the model
loaded_model = load(open('titanic_model.pkl', 'rb'))


loaded_model.fit(X_train, y_train)

y_pred = loaded_model.predict(X_test)

"""Precision

Definition: Out of all the passengers we predicted would survive, what proportion actually survived?
Focus: Minimizing false positives (predicting survival when they didn't).

Recall

Definition: Out of all the passengers who actually survived, what proportion did we correctly predict would survive?
Focus: Minimizing false negatives (predicting death when they survived).

What is Cross-Validation?

Cross-validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves splitting the dataset into multiple folds (subsets) and iteratively training and testing the model on different combinations of these folds

Why is Cross-Validation Important in Binary Classification?

Cross-validation offers several advantages, especially in binary classification tasks (where the outcome is one of two classes):

Reduced Overfitting: By training and testing on different subsets, cross-validation helps to detect and prevent overfitting. Overfitting occurs when a model performs well on the training data but poorly on unseen data.
Improved Model Selection: Cross-validation enables you to compare different models or hyperparameter settings by evaluating their performance across multiple folds. This leads to a more informed choice of the best model for your task.
More Reliable Performance Estimate: Averaging performance metrics across multiple folds provides a more reliable and less biased estimate of the model's generalization ability (how well it will perform on new data).
Better Utilization of Data: Cross-validation allows you to utilize your entire dataset for both training and testing, providing a more comprehensive evaluation.
"""